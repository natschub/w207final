{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this porject a dataset containing crime statistics in the San Francisco area from X to Y is analyzed with the goal of creating a model to predict the nature of a crime based only on its time and location. \n",
    "\n",
    "More information on the project and its dataset can be found here:\n",
    "https://www.kaggle.com/c/sf-crime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys, os\n",
    "import pandas\n",
    "import numpy as np\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n",
    "from pandas.tseries.holiday import USFederalHolidayCalendar as holidaysCalendar\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Loading data into training, dev, and test sets using headings provided from the sample submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define header (from sample submission)\n",
    "header = [\"ID\", \"ARSON\", \"ASSAULT\", \"BAD CHECKS\", \"BRIBERY\", \"BURGLARY\",\n",
    "          \"DISORDERLY CONDUCT\", \"DRIVING UNDER THE INFLUENCE\", \"DRUG/NARCOTIC\",\n",
    "          \"DRUNKENNESS\", \"EMBEZZLEMENT\", \"EXTORTION\", \"FAMILY OFFENSES\",\n",
    "          \"FORGERY/COUNTERFEITING\", \"FRAUD\", \"GAMBLING\", \"KIDNAPPING\", \"LARCENY/THEFT\",\n",
    "          \"LIQUOR LAWS\", \"LOITERING\", \"MISSING PERSON\", \"NON-CRIMINAL\", \"OTHER OFFENSES\",\n",
    "          \"PORNOGRAPHY/OBSCENE MAT\", \"PROSTITUTION\", \"RECOVERED VEHICLE\", \"ROBBERY\",\n",
    "          \"RUNAWAY\", \"SECONDARY CODES\", \"SEX OFFENSES FORCIBLE\", \"SEX OFFENSES NON FORCIBLE\",\n",
    "          \"STOLEN PROPERTY\", \"SUICIDE\", \"SUSPICIOUS OCC\", \"TREA\", \"TRESPASS\", \"VANDALISM\",\n",
    "          \"VEHICLE THEFT\", \"WARRANTS\", \"WEAPON LAWS\"]\n",
    "\n",
    "# Load training data\n",
    "df = pandas.read_csv(\"train.csv\")\n",
    "# Drop duplicates\n",
    "df = df.drop_duplicates()\n",
    "\n",
    "# Binarize with dummy variables\n",
    "dummies_df = df[['DayOfWeek', 'PdDistrict']]\n",
    "dummies_df = pandas.get_dummies(dummies_df)\n",
    "\n",
    "# Format datetime\n",
    "## Time of day\n",
    "df['Time'] = pandas.to_datetime(df['Dates']).dt.hour\n",
    "## Week of year\n",
    "df['Week'] = pandas.to_datetime(df['Dates']).dt.week\n",
    "## Holiday\n",
    "#cal = holidaysCalendar()\n",
    "#holidays = cal.holidays(start=pandas.to_datetime(df['Dates']).min(), end=pandas.to_datetime(df['Dates']).max())\n",
    "#df['Holiday'] = pandas.to_datetime(df['Dates']).isin(holidays)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df = df.drop(['Address', 'Resolution', 'Descript', 'Dates', 'DayOfWeek', 'PdDistrict'],axis=1)\n",
    "\n",
    "# Format Coords\n",
    "df['Y'] = df['Y'].apply(lambda x: round(x,4))\n",
    "df['X'] = df['X'].apply(lambda x: round(x,4))\n",
    "#df['Coords'] = df['X'].astype(str) + \", \" + df['Y'].astype(str)\n",
    "\n",
    "parsed_df = df.join(dummies_df)\n",
    "\n",
    "# Split into data/labels, train/dev\n",
    "train_data = parsed_df[parsed_df.columns.difference(['Category'])][:50000]\n",
    "train_labels = parsed_df['Category'][:50000]\n",
    "\n",
    "dev_data = parsed_df[parsed_df.columns.difference(['Category'])][-5000:]\n",
    "dev_labels = parsed_df['Category'][-5000:]\n",
    "\n",
    "# Labels list\n",
    "labels = list(set(train_labels))\n",
    "train_labels = train_labels.apply(lambda x: labels.index(x))\n",
    "dev_labels = dev_labels.apply(lambda x: labels.index(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load test data\n",
    "df_test = pandas.read_csv(\"test.csv\")\n",
    "# Drop duplicates\n",
    "df_test = df_test.drop_duplicates()\n",
    "\n",
    "# Binarize with dummy variables\n",
    "dummies_df = df_test[['DayOfWeek', 'PdDistrict']]\n",
    "dummies_df = pandas.get_dummies(dummies_df)\n",
    "\n",
    "# Format datetime\n",
    "## Time of day\n",
    "df_test['Time'] = pandas.to_datetime(df_test['Dates']).dt.hour\n",
    "## Week of year\n",
    "df_test['Week'] = pandas.to_datetime(df_test['Dates']).dt.week\n",
    "## Holiday\n",
    "#cal = holidaysCalendar()\n",
    "#holidays = cal.holidays(start=pandas.to_datetime(df['Dates']).min(), end=pandas.to_datetime(df['Dates']).max())\n",
    "#df['Holiday'] = pandas.to_datetime(df['Dates']).isin(holidays)\n",
    "\n",
    "# Drop unnecessary columns\n",
    "df_test = df_test.drop(['Address', 'Dates', 'DayOfWeek', 'PdDistrict', 'Id'],axis=1)\n",
    "\n",
    "# Format Coords\n",
    "df_test['Y'] = df_test['Y'].apply(lambda x: round(x,4))\n",
    "df_test['X'] = df_test['X'].apply(lambda x: round(x,4))\n",
    "#df_test['Coords'] = df_test['X'].astype(str) + \", \" + df_test['Y'].astype(str)\n",
    "\n",
    "parsed_df = df_test.join(dummies_df)\n",
    "\n",
    "# Clean\n",
    "df = dummies_df = parsed_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Train and fit a K-NN classifier using training data. Predict test set labels and output to a compressed CSV file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Train KNN Classifier\n",
    "neigh = KNeighborsClassifier(n_neighbors=15)\n",
    "\n",
    "# Fit\n",
    "neigh.fit(train_data, train_labels) \n",
    "\n",
    "# Predict\n",
    "result = neigh.predict(test_data)\n",
    "result = pandas.DataFrame(result)\n",
    "result = pandas.get_dummies(result, prefix='', prefix_sep='')\n",
    "# Add null categories to make kaggle happy\n",
    "result = result.T.reindex(header).T.fillna(0)\n",
    "\n",
    "# Output\n",
    "result.to_csv(\"output_knn.csv\", compression='gzip', chunksize=1000)\n",
    "\n",
    "# Return score\n",
    "#neigh.score(dev_data, dev_labels)\n",
    "\n",
    "# Train RandomForestClassifier\n",
    "#rfc = RandomForestClassifier(n_estimators=10, max_depth=None, min_samples_split=2, random_state=0)\n",
    "# Fit\n",
    "#rfc.fit(train_data, train_labels)\n",
    "# Predict\n",
    "#rfc.score(dev_data, dev_labels)\n",
    "\n",
    "#result = rfc.predict(test_data)\n",
    "#result = pandas.DataFrame(result)\n",
    "#result = pandas.get_dummies(result)\n",
    "\n",
    "#result.to_csv(\"output_rfc.csv\", compression='gzip', chunksize=1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Neural-Network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Features = 21\n",
      "Classes = 37\n",
      "Train set = 50000\n",
      "Test set = 5000\n"
     ]
    }
   ],
   "source": [
    "numFeatures = len(train_data.columns)\n",
    "numClasses = len(labels)\n",
    "numSamples = len(train_data)\n",
    "numTestExamples = len(dev_data)\n",
    "\n",
    "print('Features = %d' %(numFeatures))\n",
    "print('Classes = %d' %(numClasses))\n",
    "print('Train set = %d' %(numSamples))\n",
    "print('Test set = %d' %(numTestExamples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1\n",
      "2\n",
      "3\n",
      "4\n",
      "5\n",
      "6\n",
      "7\n",
      "8\n",
      "9\n",
      "10\n",
      "11\n",
      "12\n",
      "13\n",
      "14\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'test_data' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-c6ea6baa5cb1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     56\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     57\u001b[0m     \u001b[0mprediction\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0margmax\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtest_preds_r\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 58\u001b[1;33m     \u001b[0moutput\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprediction\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0meval\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfeed_dict\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m{\u001b[0m\u001b[0mx_\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mtest_data\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m}\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     59\u001b[0m     \u001b[0mprint\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;34m\"hi\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'test_data' is not defined"
     ]
    }
   ],
   "source": [
    "# (1) Parameters\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Constants\n",
    "testX = tf.constant(dev_data.values, dtype=tf.float32)\n",
    "hiddenlayer1_size = 2\n",
    "hiddenlayer2_size = 1\n",
    "miniBatchSize = 1\n",
    "\n",
    "# placeholders\n",
    "x_ = tf.placeholder(tf.float32, shape=[None, numFeatures], name='x')\n",
    "y_ = tf.placeholder(tf.int32, shape=[None], name='y')\n",
    "\n",
    "# and Variables\n",
    "w1 = tf.get_variable('w1', shape=[numFeatures, hiddenlayer1_size])\n",
    "b1 = tf.get_variable('b1', shape=[hiddenlayer1_size])\n",
    "w2 = tf.get_variable('w2', shape=[hiddenlayer1_size, numClasses])\n",
    "b2 = tf.get_variable('b3', shape=[numClasses])\n",
    "\n",
    "\n",
    "# (2) Model\n",
    "def model(input_layer):\n",
    "    hidden_layer1 = tf.nn.sigmoid(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "\n",
    "# (2) Model\n",
    "def model_r(input_layer):\n",
    "    hidden_layer1 = tf.nn.relu(tf.matmul(input_layer, w1) + b1)\n",
    "    output_layer = tf.nn.softmax(tf.matmul(hidden_layer1, w2) + b2)\n",
    "    return output_layer\n",
    "    \n",
    "\n",
    "# (3) Cost\n",
    "def cost(data, labels):\n",
    "    cc = tf.sqrt(tf.square(labels - model(data)))\n",
    "    return  cc\n",
    "\n",
    "# (4) Ojbective (and solver)\n",
    "y_one_hot = tf.one_hot(y_, numClasses)\n",
    "cc = cost(x_, y_one_hot)\n",
    "gd = tf.train.GradientDescentOptimizer(0.1)\n",
    "step = gd.minimize(cc)\n",
    "test_preds = model(testX)\n",
    "test_preds_r = model_r(testX)\n",
    "output = \"\"\n",
    "with tf.Session() as sess:\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    cost_vec = []\n",
    "    cost_vec_r = []\n",
    "    for i in range(15):\n",
    "        print (i)\n",
    "        for start, end in zip(range(0, numSamples, miniBatchSize), range(miniBatchSize, numSamples, miniBatchSize)):\n",
    "            batch = train_data.values[start:end], train_labels[start:end]\n",
    "            _, cost, test__preds_r = sess.run([step, cc, test_preds_r], feed_dict={x_: batch[0], y_: batch[1]})\n",
    "    \n",
    "    prediction=tf.argmax(test_preds_r,axis=1)\n",
    "    output = prediction.eval(feed_dict={x_: test_data.values})\n",
    "    print (\"hi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
